---
title: "README.md"
output: pdf_document
date: "2024-02-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# Notes on useful papers

## Adam
Modified SGD algorithm intended to address the vanishing gradient problem.
https://arxiv.org/abs/1412.6980



```{r}


# Load data
data <- read.csv("python/mnist_train.csv")

# Convert data to matrix
data <- t(as.matrix(data))

# Get dimensions of data
mm <- ncol(data)
n <- nrow(data)


X_train <- data[,2:785] / 255
Y_train <- data[,1]

# Define functions
get_predictions <- function(A2) {
  apply(A2, 2, which.max)
}

get_accuracy <- function(predictions, Y) {
  mean(predictions == Y)
}

one_hot <- function(Y) {
  n_classes <- max(Y) + 1
  one_hot_Y <- matrix(0, nrow = n_classes, ncol = length(Y))
  one_hot_Y[cbind(Y + 1, seq_along(Y))] <- 1
  one_hot_Y
}

init_params <- function(n_classes, n_features, n_hidden) {
  W1 <- matrix(runif(n_hidden * n_features) - 0.5, nrow = n_hidden, ncol = n_features)
  b1 <- matrix(runif(n_hidden) - 0.5, nrow = n_hidden, ncol = 1)
  W2 <- matrix(runif(n_classes * n_hidden) - 0.5, nrow = n_classes, ncol = n_hidden)
  b2 <- matrix(runif(n_classes) - 0.5, nrow = n_classes, ncol = 1)
  list(W1, b1, W2, b2)
}

ReLU <- function(Z) {
  pmax(Z, 0)
}

softmax <- function(Z) {
  max_Z <- apply(Z, 1, max)
  exp_Z <- exp(Z - max_Z)
  exp_Z / rowSums(exp_Z)
}

ReLU_deriv <- function(Z) {
  Z > 0
}

adam_optimizer <- function(params, gradients, learning_rate, beta1, beta2, epsilon, t) {
  m <- params$m
  v <- params$v
  
  for (i in 1:length(gradients)) {
    m[i] <- beta1 * m[i] + (1 - beta1) * gradients[i]
    v[i] <- beta2 * v[i] + (1 - beta2) * gradients[i]^2
    m_hat <- m[i] / (1 - beta1^t)
    v_hat <- v[i] / (1 - beta2^t)
    params$params[i] <- params$params[i] - learning_rate * m_hat / (sqrt(v_hat) + epsilon)
  }
  
  params$m <- m
  params$v <- v
  
  return(params)
}

# Initialize parameters
n_classes <- 10
n_features <- 784
n_hidden <- 10
params <- init_params(n_classes, n_features, n_hidden)
W1 <- params[[1]]
b1 <- params[[2]]
W2 <- params[[3]]
b2 <- params[[4]]

# Initialize Adam optimizer parameters
m <- rep(0, length(c(W1, b1, W2, b2)))
v <- rep(0, length(c(W1, b1, W2, b2)))
t <- 0

# Set learning rate and number of iterations
learning_rate <- 0.001
beta1 <- 0.9
beta2 <- 0.999
epsilon <- 1e-8
num_iterations <- 100

# Train model
for (i in 1:num_iterations) {
  # Forward pass
  Z1 <- W1 %*% X_train + b1
  A1 <- ReLU(Z1)
  Z2 <- W2 %*% A1 + b2
  A2 <- softmax(Z2)
  
  # Backward pass
  dZ2 <- A2 - one_hot(Y_train)
  dW2 <- t(dZ2 %*% t(A1)) / m
  db2 <- rowSums(dZ2) / m
  dZ1 <- t(W2) %*% dZ2 * ReLU_deriv(Z1)
  dW1 <- t(dZ1 %*% t(X_train)) / m
  db1 <- rowSums(dZ1) / m
  
  # Update parameters using Adam optimizer
  t <- t + 1
  params <- list(params = c(W1, b1, W2, b2), m = m, v = v)
  gradients <- c(dW1, db1, dW2, db2)
  params <- adam_optimizer(params, gradients, learning_rate, beta1, beta2, epsilon, t)
  W1 <- matrix(params$params[1:n_hidden * n_features], nrow = n_hidden, ncol = n_features)
  b1 <- matrix(params$params[(n_hidden * n_features + 1):(n_hidden * n_features + n_hidden)], nrow = n_hidden, ncol = 1)
  W2 <- matrix(params$params[(n_hidden * n_features + n_hidden + 1):(n_hidden * n_features + n_hidden + n_classes * n_hidden)], nrow = n_classes, ncol = n_hidden)
  b2 <- matrix(params$params[(n_hidden * n_features + n_hidden + n_classes * n_hidden + 1):length(params$params)], nrow = n_classes, ncol = 1)
  
  # Print accuracy every 10 iterations
  if (i %% 10 == 0) {
    predictions <- get_predictions(A2)
    cat("Iteration", i, "accuracy:", get_accuracy(predictions, Y_train), "\n")
  }
}


```







Let's denote the loss function as `L`, which is a function of the predicted output `z2` and the true labels `label`. The goal is to compute the gradient of the loss function with respect to the weights of the second layer `W2`, denoted as 
$$\frac{\partial L}{\partial W_2}$$.

Using the chain rule, we can write the gradient of the loss function with respect to `W2` as:

$$\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial W_2}$$

Here, 
$$\frac{\partial L}{\partial z_2}$$

represents the gradient of the loss function with respect to the predicted output `z2`, and 

$$\frac{\partial z_2}{\partial W_2}$$ 

represents the gradient of the predicted output `z2` with respect to the weights `W2`.

Now, let's assume the loss function is a mean squared error (MSE) between the predicted output `z2` and the true labels `label`, which is a common choice in neural networks:

$$L = \frac{1}{2} \left(\text{label} - z_2\right)^2$$

Using the chain rule, we can compute the gradient of the loss function with respect to `z2` as:

$$\frac{\partial L}{\partial z_2} =  \left(\text{label} - z_2\right)$$

This is because derivative of the squared error with respect to `z2` is

$$2 \cdot \left(\text{label} - z_2\right)$$

and we divide by 2 to get the average error.

Now, we need to compute the gradient of the predicted output `z2` with respect to the weights `W2`. Assuming the neural network has a linear activation function in the second layer, we can write `z2` as:

$$z_2 = W_2 \cdot z_1 + b_2$$

where `z1` is the output of the first layer, and `b2` is the bias term in the second layer.

Using the product rule, we can compute the gradient of `z2` with respect to `W2` as:

$$\frac{\partial z_2}{\partial W_2} = z_1$$



This is because the derivative of  
$$W_2 \cdot z_1$$ with respect to `W2` is `z1`, and the derivative of `b2` with respect to `W2` is 0.

Now, we can plug in the expressions for 
$$\frac{\partial L}{\partial z_2}$$  

and 

$$\frac{\partial z_2}{\partial W_2}$$ into the chain rule formula:

$$\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial W_2} =  - \left(\text{label} - z_2\right) \cdot z_1$$

This is exactly what the line of code `dW2 <- t(z1) %*% db2_temp` is computing! The transpose of `z1` is used to ensure the correct matrix multiplication, and `db2_temp` represents the error term `label - z2`.

By computing the gradient of the loss function with respect to the weights `W2` in this way, we can update the weights using an optimization algorithm such as stochastic gradient descent (SGD) to minimize the loss function.



The chain rule is a fundamental concept in calculus that allows us to compute the derivative of a composite function. In this case, we have a composite function:

$$L = L(z_2)$$

where $L$ is the loss function, and $z_2$ is the output of the second layer.

The chain rule states that if we have a composite function $f(g(x))$, where $f$ and $g$ are both functions of $x$, then the derivative of $f$ with respect to $x$ is given by:

$$\frac{\partial f}{\partial x} = \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial x}$$

In our case, we can apply the chain rule by identifying the inner function $g$ as $z_2$, and the outer function $f$ as $L$. Specifically, we can write:

$$L = L(z_2(W_2, z_1, b_2))$$

where $z_2$ is a function of $W_2$, $z_1$, and $b_2$.

Now, we want to compute the derivative of $L$ with respect to $W_2$. Using the chain rule, we can write:

$$\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial W_2}$$

Here, $\frac{\partial L}{\partial z_2}$ is the derivative of the loss function $L$ with respect to the output $z_2$, and $\frac{\partial z_2}{\partial W_2}$ is the derivative of the output $z_2$ with respect to the weights $W_2$.

The key insight is that we can separate the computation of the derivativo parts:

1. Compute the derivative of the loss function $L$ with respect to the output $z_2$. This is a simple computation, since $L$ is a function of $z_2$ only.

$$\frac{L}{\partial z_2} = - (y - z_2)$$

where $y$ is the true label.

2. Compute the derivative of the output $z_2$ with respect to the weights $W_2$. This is also a simple computation, since $z_2$ is a function of $W_2$, $z_1$, and $b_2$.

$$\frac{\partial z_2}{\partial W_2} = z_1$$

By multiplying these two derivatives together, we get the final result:

$$\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial W_2} = - (y - z_2) \cdot z_1$$

This is exactly what the line of code `dW2 <- t(z1) %*% db2_temp` is computing! The transpose of $z_1$ is used to ensure the correct matrix multiplication, and `db2_temp` represents the error term $y - z_2$.


When computing the derivative of the loss function with respect to the weights $W_2$, we can ignore the inputs $z_1$ and $b_2$ because they are not directly affected by the weights $W_2$.

To see why, let's revisit the expression for the output $z_2$:

$z_2 = W_2 \cdot z_1 + b_2$

When computing the derivative of $z_2$ with respect to $W_2$, we only care about how $W_2$ affects the output $z_2$. The inputs $z_1$ and $b_2$ are treated as constants, because they are not functions of $W_2$.

In other words, when we compute the derivative of $z_2$ with respect to $W_2$, we are asking: "How does $W_2$ change the output $z_2$, assuming $z_1$ and $b_2$ are fixed?"

Using the product rule, we can compute the derivative of $z_2$ with respect to $W_2$ as:

$\frac{\partial z_2}{\partial W_2} = z_1$

Here, we've ignored the contribution of $b_2$ because it's a constant term that doesn't depend on $W_2$. Similarly, we've treated $z_1$ as a constant, because it's an input that's not directly affected by $W_2$.

By ignoring $z_1$ and $b_2$ in this way, we're effectively assuming that they are fixed inputs that don't change when we update the weights $W_2$. This is a reasonable assumption, because we're only interested in how the weights $W_2$ affect the output $z_2$, not how the inputs $z_1$ and $b_2$ affect the output.

Of course, this assumption is only valid if the inputs $z_1$ and $b_2$ are indeed fixed and don't depend on the weights $W_2$. In general, if the inputs do depend on the weights, we would need to take that into account when computing the derivative.

But in this specific case, ignoring $_1$ and $b_2$ allows us to focus on the direct effect of $W_2$ on the output $z_2$, which is exactly what we need to compute the gradient of the loss function with respect to the weights.


## Step 1: Compute the partial derivative of the loss with respect to yh
The partial derivative of the loss with respect to yh is computed as follows:

$$\frac{\partial L}{\partial yh} = \frac{\partial}{\partial yh} (yh - t(y))^2 = 2(yh - t(y))$$

## Step 2: Compute the partial derivative of the loss with respect to th
Using the chain rule, we can compute the partial derivative of the loss with respect to th as follows:

$$\frac{\partial L}{\partial th} = \frac{\partial L}{\partial yh} \frac{\partial yh}{\partial th} = 2(yh - t(y)) \frac{\partial}{\partial th} softmax(th)$$

## Step 3: Compute the partial derivative of the softmax function with respect to th
The partial derivative of the softmax function with respect to th is computed as follows:

$$\frac{\partial}{\partial th} softmax(th) = softmax(th) (1 - softmax(th))$$

## Step 4: Compute the partial derivative of the loss with respect to th
Substituting the result from Step 3 into the result from Step 2, we get:

$$\frac{\partial L}{\partial th} = 2(yh - t(y)) softmax(th) (1 - softmax(th))$$

## Step 5: Compute the partial derivative of the loss with respect to h
Using the chain rule, we can compute the partial derivative of the loss with respect to h as follows:

$$\frac{\partial L}{\partial h} = \frac{\partial L}{\partial th} \frac{\partial th}{\partial h} = 2(yh - t(y)) softmax(th) (1 - softmax(th)) U$$

## Step 6: Compute the partial derivative of the loss with respect to z
Using the chain rule, we can compute the partial derivative of the loss with respect to z as follows:

$$\frac{\partial L}{\partial z} = \frac{\partial L}{\partial h} \frac{\partial h}{\partial z} = 2(yh - t(y)) softmax(th) (1 - softmax(th)) U sgn(z)$$

## Step 7: Compute the partial derivative of the loss with respect to W
Using the chain rule, we can compute the partial derivative of the loss with respect to W as follows:

$$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial W} = 2(yh - t(y)) softmax(th) (1 - softmax(th)) U sgn(z) x$$

## Step 8: Compute the partial derivative of the loss with respect to b1
Using the chain rule, we can compute the partial derivative of the loss with respect to b1 as follows:

$$\frac{\partial L}{\partial b1} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial b1} = 2(yh - t(y)) softmax(th) (1 - softmax(th)) U sgn(z)$$

## Step 9: Compute the partial derivative of the loss with respect to U
Using the chain rule, we can compute the partial derivative of the loss with respect to U as follows:

$$\frac{\partial L}{\partial U} = \frac{\partial L}{\partial th} \frac{\partial th}{\partial U} = 2(yh - t(y)) softmax(th) (1 - softmax(th)) h$$

## Step 10: Compute the partial derivative of the loss with respect to b2
Using the chain rule, we can compute the partial derivative of the loss with respect to b2 as follows:

$$\frac{\partial L}{\partial b2} = \frac{\partial L}{\partial th} \frac{\partial th}{\partial b2} = 2(yh - t(y)) softmax(th) (1 - softmax(th))$$




